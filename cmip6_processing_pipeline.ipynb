{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90c3eb9-a7ba-42a9-919e-3df2f5796855",
   "metadata": {},
   "source": [
    "## README: CMIP6-LE and CMIP5-LE Processing Pipeline for Calculating Hemispheric Total Sea-Ice Area\n",
    "The code here can be used to merge .nc files based on time (1850-1950 + 1950-2014), combine ensemble members (r1,r2,...,rn) into a single .nc file, and calculate siarea in the northern and southern hemisphere. A lot of the code is copy and pasted between cells, for different models and edge cases. TODO: When I have some extra time I need to pull out repeated code into multiple purpose functions.\n",
    "\n",
    "For info on Large-Ensembles refer to: https://www.cesm.ucar.edu/projects/community-projects/MMLEA/ (outdated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bbe4442-f9d3-448d-a90a-b7565d481559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cmocean as cmo\n",
    "import xarray as xr\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# CDO warnings annoy me...\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.system(f\"module load cdo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38aae33e-9edc-452c-9e53-3554697826f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # \"ACCESS-ESM1-5\",\n",
    "    # \"ACCESS-CM2\",\n",
    "    \"CanESM5\", \n",
    "    # \"CNRM-CM6-1\",\n",
    "    # \"FGOALS-g3\",  # TODO\n",
    "    # \"IPSL-CM6A-LR\", #TODO\n",
    "    # \"MPI-ESM1-2-LR\",\n",
    "    # \"MPI-ESM1-2-HR\", \n",
    "    # \"NorESM2-LM\",\n",
    "    # \"MIROC-ES2L\",\n",
    "    # \"MIROC6\", \n",
    "]\n",
    "\n",
    "meta = {\n",
    "    \"historical\": defaultdict(lambda: {}), \n",
    "    \"ssp126\": defaultdict(lambda: {}),\n",
    "    \"ssp245\": defaultdict(lambda: {}),\n",
    "    \"ssp370\": defaultdict(lambda: {}),\n",
    "    \"ssp585\": defaultdict(lambda: {})\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da510ace-e232-4c52-bb0d-66fb99d99daf",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b61424b-c2f4-4674-b70a-80715d85a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensmem(dfiles):\n",
    "# Get ensemble members:\n",
    "    ensmem = []\n",
    "    for i in range(len(dfiles)):\n",
    "        s = re.search('(.*_r)(.*)(.*i1)', dfiles[i])\n",
    "        if s == None: continue \n",
    "        ensmem.append(s.group(2))\n",
    "    ensmem = np.unique(ensmem)\n",
    "    return ensmem, len(ensmem) \n",
    "\n",
    "def drop_duplicates(ds): \n",
    "    return xr.DataArray.drop_duplicates(ds, dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ff545-bb6c-41d2-8340-b20b5a59b5d7",
   "metadata": {},
   "source": [
    "# CMIP6 Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe4274-5800-42e4-87d3-cca3cc955d7c",
   "metadata": {},
   "source": [
    "### Step 1: Combine files that do not span entire time-period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2088110-ab9d-4ad4-b798-7b08d64f6589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Processing historical MIROC-ES2L\n",
      "Begin Processing historical MIROC6\n",
      "Ensemble Members:  ['1' '10' '11' '12' '13' '14' '15' '16' '17' '18' '19' '2' '20' '21' '22'\n",
      " '23' '24' '25' '26' '27' '28' '29' '3' '30' '31' '32' '33' '34' '35' '36'\n",
      " '37' '38' '39' '4' '40' '41' '42' '43' '44' '45' '46' '47' '48' '49' '5'\n",
      " '50' '6' '7' '8' '9']\n",
      "Begin Processing ssp126 MIROC-ES2L\n",
      "Begin Processing ssp126 MIROC6\n",
      "Begin Processing ssp245 MIROC-ES2L\n",
      "Begin Processing ssp245 MIROC6\n",
      "Ensemble Members:  ['1' '10' '11' '12' '13' '14' '15' '16' '17' '18' '19' '2' '20' '21' '22'\n",
      " '23' '24' '25' '26' '27' '28' '29' '3' '30' '31' '32' '33' '34' '35' '36'\n",
      " '37' '38' '39' '4' '40' '41' '42' '43' '44' '45' '46' '47' '48' '49' '5'\n",
      " '50' '6' '7' '8' '9']\n",
      "Begin Processing ssp370 MIROC-ES2L\n",
      "Begin Processing ssp370 MIROC6\n",
      "Begin Processing ssp585 MIROC-ES2L\n",
      "Begin Processing ssp585 MIROC6\n"
     ]
    }
   ],
   "source": [
    "for exp in meta.keys():\n",
    "    droot = \"/glade/scratch/zespinosa/cmip6/scenarios/raw_data/data/\"\n",
    "    time = \"201501-210012.nc\"\n",
    "    if exp == \"historical\": \n",
    "        droot = \"/glade/scratch/zespinosa/cmip6/historical/raw_data/data/\"\n",
    "        time = \"185001-201412.nc\"\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"Begin Processing {exp} {model}\")\n",
    "        try:  \n",
    "            fp = os.path.join(droot, f\"v_siconc_f_mon_e_{exp}_s_{model}\")\n",
    "\n",
    "            # Get ensemble members: \n",
    "            dfiles = sorted(glob(os.path.join(fp, '*.nc')))\n",
    "            if len(dfiles) != 0:\n",
    "                ensmem, nensmem = get_ensmem(dfiles)\n",
    "                meta[exp][model][\"ensmem\"] = ensmem\n",
    "                meta[exp][model][\"nensmem\"] = nensmem\n",
    "                print(\"Ensemble Members: \", ensmem)\n",
    "\n",
    "                # Combine files that do not span entire time-period and/or move into 'combined' folder\n",
    "                dest = os.path.join(fp, f\"{exp}_combined\")\n",
    "                if not os.path.exists(dest):\n",
    "                    os.mkdir(dest)\n",
    "                date = dfiles[0].split(\"_\")[-1]\n",
    "                if len(os.listdir(dest)) == 0:\n",
    "                    if date != time:\n",
    "                        for i in ensmem: \n",
    "                            sfiles = os.path.join(fp, f\"*_{model}_{exp}_r{i}i1p1f2_gn*.nc\")\n",
    "                            sp = os.path.join(dest,f\"siconc_SImon_{model}_{exp}_r{i}i1p1f2_gn_{time}\")\n",
    "                            print(f\"Creating {sp}\")\n",
    "                            os.system(f\"module load cdo; cdo -s mergetime {sfiles} {sp}\")\n",
    "                    else: \n",
    "                        print(f\"Moving files to {dest}\")\n",
    "                        for f in dfiles: \n",
    "                            os.system(f\"mv {f} {dest}\")\n",
    "        except Exception as err:\n",
    "            print(f\"{model} Failed! \\n {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261610e3-0a12-464a-aa2b-27cedd35e5e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Calculate SIA for historical and scenarios and combine based on ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51b68402-d435-4dba-8395-206304e0470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin processing for model: CanESM5 \n",
      "\n",
      "  # historical ensemble members = 25\n",
      "  Area filename: /glade/scratch/zespinosa/cmip6LE/historical/area/data/areacello_Ofx_CanESM5_historical_r1i1p1f1_gn.nc\n",
      "  Spatial dimension names: ('j', 'i')\n",
      "  Index of middle latitude (rounded): 145\n",
      "  Latitudes start from: -78.39350128173828\n",
      "  Beginning historical siarea calculation . . .\n",
      "  Done with historical siarea calculation \n",
      "  # ssp126 ensemble members = 25\n",
      "  Beginning ssp126 siarea calculation . . .\n",
      "  Done with ssp126 siarea calculation \n",
      "  # overlapping ensemble members = 25\n",
      "  Saving to cdf . . .\n",
      "  Arctic filename historical ssp126: siarea_nh_historical_ssp126_CanESM5i1p1f1.nc\n",
      "  Antarctic filename historical ssp126: siarea_sh_historical_ssp126_CanESM5i1p1f1.nc\n",
      "  # ssp245 ensemble members = 25\n",
      "  Beginning ssp245 siarea calculation . . .\n",
      "  Done with ssp245 siarea calculation \n",
      "  # overlapping ensemble members = 25\n",
      "  Saving to cdf . . .\n",
      "  Arctic filename historical ssp245: siarea_nh_historical_ssp245_CanESM5i1p1f1.nc\n",
      "  Antarctic filename historical ssp245: siarea_sh_historical_ssp245_CanESM5i1p1f1.nc\n",
      "  # ssp370 ensemble members = 25\n",
      "  Beginning ssp370 siarea calculation . . .\n",
      "  Done with ssp370 siarea calculation \n",
      "  # overlapping ensemble members = 25\n",
      "  Saving to cdf . . .\n",
      "  Arctic filename historical ssp370: siarea_nh_historical_ssp370_CanESM5i1p1f1.nc\n",
      "  Antarctic filename historical ssp370: siarea_sh_historical_ssp370_CanESM5i1p1f1.nc\n",
      "  # ssp585 ensemble members = 25\n",
      "  Beginning ssp585 siarea calculation . . .\n",
      "  Done with ssp585 siarea calculation \n",
      "  # overlapping ensemble members = 25\n",
      "  Saving to cdf . . .\n",
      "  Arctic filename historical ssp585: siarea_nh_historical_ssp585_CanESM5i1p1f1.nc\n",
      "  Antarctic filename historical ssp585: siarea_sh_historical_ssp585_CanESM5i1p1f1.nc\n",
      "  Done with CanESM5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models: \n",
    "    # try: \n",
    "        print(f\"Begin processing for model: {model} \\n\")\n",
    "\n",
    "        rootHist = f\"/glade/scratch/zespinosa/cmip6LE/historical/raw_data/data/v_siconc_f_mon_e_historical_s_{model}/historical_combined/\" \n",
    "        histFiles = sorted(glob(rootHist + '/*p1f1_gn*.nc'))\n",
    "\n",
    "        print(f\"  # historical ensemble members = {len(histFiles)}\")\n",
    "\n",
    "        # Get ensemble members\n",
    "        ensHist, nensHist = get_ensmem(histFiles)\n",
    "\n",
    "        # Load historical data\n",
    "        mfds = xr.open_mfdataset(histFiles, combine='nested', concat_dim='ensmem', preprocess=drop_duplicates)\n",
    "        mfds = mfds.assign_coords({'ensmem': ('ensmem', np.arange(0, len(histFiles)))})\n",
    "\n",
    "        # Get grid cell area file\n",
    "        ddir = '/glade/scratch/zespinosa/cmip6LE/historical/area/data/'\n",
    "        dfile = sorted(glob(os.path.join(ddir, \"areacello*\"+model+\"*.nc\")))[0]\n",
    "        print(f\"  Area filename: {dfile}\")\n",
    "        ncf = os.path.join(ddir, dfile)\n",
    "        ds = xr.open_dataset(ncf)\n",
    "\n",
    "        area = ds['areacello']\n",
    "        siconc = mfds['siconc']/100\n",
    "\n",
    "        # Compute sea ice area in each grid cell\n",
    "        siarea = siconc*area.data[None, None, :, :]\n",
    "        siarea = siarea.rename('siarea')\n",
    "\n",
    "        # Get name of lat and lon dimensions, these differ by model\n",
    "        spatial_dims = siarea.dims[-2:]\n",
    "        print(f\"  Spatial dimension names: {spatial_dims}\")\n",
    "\n",
    "        # Get index of approximate middle of latitudes to separate Arctic and Antarctic\n",
    "        lat_mid_index = int(siarea.shape[2]/2)\n",
    "        print(f\"  Index of middle latitude (rounded): {lat_mid_index}\")\n",
    "\n",
    "        # Get first latitude to determine if lats start in Arctic or Antarctic\n",
    "        # First need to get name of latitude variable, differs by model\n",
    "        possible_latnames = [\"lat\", \"latitude\", \"nav_lat\"]\n",
    "        for name in possible_latnames:\n",
    "            if name in ds.coords:\n",
    "                latname = name\n",
    "\n",
    "        lat0 = siarea[latname][0].compute()[0]\n",
    "\n",
    "        print(f\"  Latitudes start from: {lat0.data}\")\n",
    "        print(\"  Beginning historical siarea calculation . . .\")\n",
    "\n",
    "        if lat0 < 0: # If latitudes start from Antarctica\n",
    "            siarea_nh_hist = siarea[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "            siarea_sh_hist = siarea[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "        else: # If latitudes start from Arctic\n",
    "            siarea_nh_hist = siarea[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "            siarea_sh_hist = siarea[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "\n",
    "        print(\"  Done with historical siarea calculation \")\n",
    "\n",
    "        for exp in meta.keys():\n",
    "            dest = \"/glade/scratch/zespinosa/cmip6LE/processed\"\n",
    "            dout_nh = f'siarea_nh_historical_{exp}_'+model+'i1p1f1.nc'\n",
    "            dout_sh = f'siarea_sh_historical_{exp}_'+model+'i1p1f1.nc'\n",
    "            output_nh = os.path.join(dest,dout_nh)\n",
    "            output_sh = os.path.join(dest,dout_sh)\n",
    "            \n",
    "            if os.path.exists(output_nh) and os.path.exists(output_sh): continue\n",
    "            if exp == \"historical\": continue\n",
    "\n",
    "            rootScen = f\"/glade/scratch/zespinosa/cmip6LE/scenarios/raw_data/data/v_siconc_f_mon_e_{exp}_s_{model}/{exp}_combined/\" \n",
    "            scenFiles = sorted(glob(rootScen + '/*p1f1_gn*.nc'))\n",
    "\n",
    "            # Get ensemble members\n",
    "            ensScen, nensScen = get_ensmem(scenFiles)\n",
    "            print(f\"  # {exp} ensemble members = {nensScen}\")\n",
    "\n",
    "\n",
    "            # Load scenario data\n",
    "            mfdsScen = xr.open_mfdataset(scenFiles, combine='nested', concat_dim='ensmem', preprocess=drop_duplicates)\n",
    "            mfdsScen = mfdsScen.assign_coords({'ensmem': ('ensmem', np.arange(0, len(scenFiles)))})\n",
    "\n",
    "            siconcScen = mfdsScen['siconc']/100\n",
    "\n",
    "            # Compute sea ice area in each grid cell\n",
    "            siareaScen = siconcScen*area.data[None, None, :, :]\n",
    "            siareaScen = siareaScen.rename('siarea')\n",
    "\n",
    "            print(f\"  Beginning {exp} siarea calculation . . .\")\n",
    "\n",
    "            if lat0 < 0: # If latitudes start from Antarctica\n",
    "                siarea_nh_scen = siareaScen[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "                siarea_sh_scen = siareaScen[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "            else: # If latitudes start from Arctic\n",
    "                siarea_nh_scen = siareaScen[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "                siarea_sh_scen = siareaScen[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "\n",
    "            print(f\"  Done with {exp} siarea calculation \")\n",
    "\n",
    "            # Get overlapping ensemble members of scenario and historical\n",
    "            overlap = list(set(ensScen) & set(ensHist))\n",
    "            histMembers = [list(ensHist).index(i) for i in overlap]\n",
    "            scenMembers = [list(ensScen).index(i) for i in overlap]\n",
    "            \n",
    "            print(f\"  # overlapping ensemble members = {len(overlap)}\")\n",
    "\n",
    "            siarea_nh = xr.concat([siarea_nh_hist[histMembers,:],siarea_nh_scen[scenMembers,:]], join=\"override\", dim=\"time\")\n",
    "            siarea_sh = xr.concat([siarea_sh_hist[histMembers,:],siarea_sh_scen[scenMembers,:]], join=\"override\", dim=\"time\")\n",
    "\n",
    "                \n",
    "            print(\"  Saving to cdf . . .\")\n",
    "            print(f\"  Arctic filename historical {sp}: {dout_nh}\")            \n",
    "            siarea_nh.to_dataset().to_netcdf(output_nh)\n",
    "            print(f\"  Antarctic filename historical {sp}: {dout_sh}\")\n",
    "            siarea_sh.to_dataset().to_netcdf(output_sh)\n",
    "    \n",
    "    # except Exception as err:\n",
    "        # print(f\"{model} Failed! \\n {err}\")\n",
    "            \n",
    "        print(f\"  Done with {model}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbcc4a-6e7c-4521-95ad-1f75d19d0406",
   "metadata": {},
   "source": [
    "# CESM2-LE Processing from Casper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "484514e3-eff0-4164-875d-67ed2db21ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Load historical CESM2 Files\n",
      "  Load SSP370 CESM2 Files\n",
      "  Area filename: /glade/scratch/zespinosa/cmip6/CESM2/areacello_CESM2.nc\n",
      "  Spatial dimension names: ('nj', 'ni')\n",
      "  Index of middle latitude (rounded): 192\n",
      "  Latitudes start from: -79.22052001953125\n",
      "  Beginning siarea calculation . . .\n",
      "  Done with siarea calculation \n",
      "  Saving to cdf . . .\n",
      "  Arctic filename historical: /glade/scratch/zespinosa/cmip6/processed/siarea_nh_historical_ssp370_CESM2.nc\n",
      "  Antarctic filename historical: /glade/scratch/zespinosa/cmip6/processed/siarea_sh_historical_ssp370_CESM2.nc\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "droot = \"/glade/scratch/zespinosa/cmip6/CESM2/\"\n",
    "\n",
    "# Download historical files\n",
    "print(f\"  Load historical CESM2 Files\")\n",
    "dFiles = sorted(glob(droot + 'siarea_aice_BHISTcmip6*_CESM2.nc'))\n",
    "mfdsHist = xr.open_mfdataset(dFiles, combine='nested', concat_dim='ensmem', preprocess=drop_duplicates)\n",
    "mfdsHist = mfdsHist.assign_coords({'ensmem': ('ensmem', np.arange(0, len(dFiles)))})\n",
    "\n",
    "# Download scenario files\n",
    "print(f\"  Load SSP370 CESM2 Files\")\n",
    "dFiles = sorted(glob(droot + 'siarea_aice_BSSP370cmip6*_CESM2.nc'))\n",
    "mfdsSSP370 = xr.open_mfdataset(dFiles, combine='nested', concat_dim='ensmem', preprocess=drop_duplicates)\n",
    "mfdsSSP370 = mfdsSSP370.assign_coords({'ensmem': ('ensmem', np.arange(0, len(dFiles)))})\n",
    "\n",
    "aiceHist = mfdsHist['aice']\n",
    "aiceSSP370 = mfdsSSP370['aice']\n",
    "\n",
    "# Get grid cell area file\n",
    "ncf = sorted(glob(os.path.join(droot, \"areacello*.nc\")))[0]\n",
    "print(f\"  Area filename: {ncf}\")\n",
    "ds = xr.open_dataset(ncf)\n",
    "\n",
    "area = ds['areacello']\n",
    "\n",
    "# Get name of lat and lon dimensions, these differ by model\n",
    "spatial_dims = aiceHist.dims[-2:]\n",
    "print(f\"  Spatial dimension names: {spatial_dims}\")\n",
    "\n",
    "# Get index of approximate middle of latitudes to separate Arctic and Antarctic\n",
    "lat_mid_index = int(aiceHist.shape[2]/2)\n",
    "print(f\"  Index of middle latitude (rounded): {lat_mid_index}\")\n",
    "\n",
    "# Get first latitude to determine if lats start in Arctic or Antarctic\n",
    "# First need to get name of latitude variable, differs by model\n",
    "lat0 = aiceHist['TLAT'][0, 0].compute()\n",
    "\n",
    "print(f\"  Latitudes start from: {lat0.data}\")\n",
    "print(\"  Beginning siarea calculation . . .\")\n",
    "\n",
    "# Compute sea ice area in each grid cell\n",
    "siareaHist = aiceHist*area.data[None, None, :, :]\n",
    "siareaHist = siareaHist.rename('siarea')\n",
    "\n",
    "siareaSSP370 = aiceSSP370*area.data[None, None, :, :]\n",
    "siareaSSP370 = siareaSSP370.rename('siarea')\n",
    "\n",
    "if lat0 < 0: # If latitudes start from Antarctica\n",
    "    siarea_nh_ssp370 = siareaSSP370[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "    siarea_sh_ssp370 = siareaSSP370[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "    siarea_nh_hist = siareaHist[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "    siarea_sh_hist = siareaHist[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "else: # If latitudes start from Arctic\n",
    "    siarea_nh_ssp370 = siareaSSP370[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "    siarea_sh_ssp370 = siareaSSP370[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "    siarea_nh_hist = siareaHist[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "    siarea_sh_hist = siareaHist[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "\n",
    "print(f\"  Done with siarea calculation \")\n",
    "\n",
    "siarea_nh = xr.concat([siarea_nh_hist,siarea_nh_ssp370], join=\"override\", dim=\"time\")\n",
    "siarea_sh = xr.concat([siarea_sh_hist,siarea_sh_ssp370], join=\"override\", dim=\"time\")\n",
    "\n",
    "sp = \"/glade/scratch/zespinosa/cmip6/processed\"\n",
    "dout_nh = os.path.join(sp, \"siarea_nh_historical_ssp370_CESM2.nc\")\n",
    "dout_sh = os.path.join(sp, \"siarea_sh_historical_ssp370_CESM2.nc\")\n",
    "print(\"  Saving to cdf . . .\")\n",
    "\n",
    "print(f\"  Arctic filename historical: {dout_nh}\")            \n",
    "siarea_nh.to_dataset().to_netcdf(dout_nh)\n",
    "print(f\"  Antarctic filename historical: {dout_sh}\")\n",
    "siarea_sh.to_dataset().to_netcdf(dout_sh)\n",
    "print(\"Done!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7ddc2-8e10-41df-8cb4-fd085e9f7330",
   "metadata": {},
   "source": [
    "# CMIP5-LE\n",
    "Process code at '/glade/collections/cdg/data/CLIVAR_LE' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe8e666-3b11-4585-918b-8f7826e8f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsCMIP5 = {\n",
    "    # \"CSIRO-Mk3-6-0\": \"csiro_mk36_lens\",     # NEED areacello  (96x192)\n",
    "    # \"CanESM2\": \"canesm2_lens\",              # DONE\n",
    "    # \"GFDL-CM3\": \"gfdl_cm3_lens\",            # DONE\n",
    "    # \"GFDL-ESM2M\": \"gfdl_esm2m_lens\",        # NEED areacello (180x360)\n",
    "    # \"CESM1-CAM5\": \"cesm_lens\",                # DONE\n",
    "    # \"MPI-ESM\": \"mpi_lens\",                  # Combine Historic and RCP (good with areacello)\n",
    "}\n",
    "exps = [\"rcp85\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a48aed0-2361-455b-ab92-009151e1d377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Load GFDL-CM3 rcp85 files 20\n",
      "  Area filename: /glade/collections/cdg/data/CLIVAR_LE/gfdl_cm3_lens/fx/areacello/areacello_fx_GFDL-CM3_historical_r0i0p0.nc\n",
      "  Spatial dimension names: ('rlat', 'rlon')\n",
      "  Index of middle latitude (rounded): 100\n",
      "  Latitudes start from: -81.5\n",
      "  Beginning siarea calculation . . .\n",
      "  Saving to cdf . . .\n",
      "  Arctic filename historical: /glade/scratch/zespinosa/cmip6LE/processed/siarea_nh_historical_rcp85_GFDL-CM3_192001-210012.nc\n",
      "  Antarctic filename historical: /glade/scratch/zespinosa/cmip6LE/processed/siarea_sh_historical_rcp85_GFDL-CM3_192001-210012.nc\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "droot = \"/glade/collections/cdg/data/CLIVAR_LE\"\n",
    "dsave = \"/glade/scratch/zespinosa/cmip6LE/processed\"\n",
    "\n",
    "for model, mpath in modelsCMIP5.items():\n",
    "    for exp in exps: \n",
    "        try: \n",
    "            # Load Datafiles\n",
    "            dFiles = sorted(glob(os.path.join(droot, mpath, \"OImon\", \"sic\", f\"*{model}*{exp}*192001-210012.nc\")))\n",
    "            print(f\"  Load {model} {exp} files {len(dFiles)}\")\n",
    "            mfds = xr.open_mfdataset(dFiles, combine='nested', concat_dim='ensmem', preprocess=drop_duplicates)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            \n",
    "#             dFilesHist = sorted(glob(os.path.join(droot, mpath, \"OImon\", \"sic\", f\"*{model}*historical*.nc\")))\n",
    "#             print(f\"  Load {model} hist files {len(dFilesHist)}\")\n",
    "#             mfdsHist = xr.open_mfdataset(dFilesHist, combine='nested', concat_dim='ensmem', preprocess=drop_duplicates)\n",
    "            \n",
    "#             mfds = xr.concat([mfdsHist,mfdsRCP], join=\"override\", dim=\"time\")\n",
    "            \n",
    "            # Get grid cell area file\n",
    "            # ncf = \"/glade/scratch/zespinosa/cmip5/area/areacello_CanESM2.nc\"\n",
    "            ncf = sorted(glob(os.path.join(droot, mpath, \"fx\", \"areacello\", \"*areacello*.nc\")))[0]\n",
    "            print(f\"  Area filename: {ncf}\")\n",
    "            ds = xr.open_dataset(ncf)\n",
    "            \n",
    "            aice = mfds['sic']/100\n",
    "            area = ds['areacello']\n",
    "\n",
    "            # Get name of lat and lon dimensions, these differ by model\n",
    "            spatial_dims = aice.dims[-2:]\n",
    "            print(f\"  Spatial dimension names: {spatial_dims}\")\n",
    "\n",
    "            # Get index of approximate middle of latitudes to separate Arctic and Antarctic\n",
    "            lat_mid_index = int(aice.shape[2]/2)\n",
    "            print(f\"  Index of middle latitude (rounded): {lat_mid_index}\")\n",
    "\n",
    "            # Get first latitude to determine if lats start in Arctic or Antarctic\n",
    "            # First need to get name of latitude variable, differs by model\n",
    "            possible_latnames = [\"j\",\"rlat\", \"lat\", \"latitude\", \"nav_lat\"]\n",
    "            for name in possible_latnames:\n",
    "                if name in ds.coords:\n",
    "                    latname = name\n",
    "                    break\n",
    "\n",
    "            lat0 = aice[latname][0].compute()\n",
    "\n",
    "            print(f\"  Latitudes start from: {lat0.data}\")\n",
    "            print(\"  Beginning siarea calculation . . .\")\n",
    "\n",
    "            # Compute sea ice area in each grid cell\n",
    "            siarea = aice*area.data[None, None, :, :]\n",
    "            siarea = siarea.rename('siarea')\n",
    "\n",
    "            if lat0 < 0: # If latitudes start from Antarctica\n",
    "                siarea_nh = siarea[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "                siarea_sh = siarea[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "            else: # If latitudes start from Arctic\n",
    "                siarea_nh = siarea[:, :, :lat_mid_index, :].sum(spatial_dims) #.compute()\n",
    "                siarea_sh = siarea[:, :, lat_mid_index:, :].sum(spatial_dims) #.compute()\n",
    "\n",
    "            time = dFiles[0].split(\"_\")[-1]\n",
    "            dout_nh = os.path.join(dsave, f\"siarea_nh_historical_{exp}_{model}_192001-210012.nc\")\n",
    "            dout_sh = os.path.join(dsave, f\"siarea_sh_historical_{exp}_{model}_192001-210012.nc\")\n",
    "            \n",
    "            print(\"  Saving to cdf . . .\")\n",
    "            print(f\"  Arctic filename historical: {dout_nh}\")            \n",
    "            siarea_nh.to_dataset().to_netcdf(dout_nh)\n",
    "            print(f\"  Antarctic filename historical: {dout_sh}\")\n",
    "            siarea_sh.to_dataset().to_netcdf(dout_sh)\n",
    "            print(\"Done!\") \n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"{model} Failed! \\n {err}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb99fe4-a3f5-4170-a641-25db0effd15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
